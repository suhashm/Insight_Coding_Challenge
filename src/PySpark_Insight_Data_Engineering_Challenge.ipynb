{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# **Insight Data Engineering Coding Challenge**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Task 1: Word Frequency count in the tweets**\n",
    "####  In this task word frequency is efficiently computed using distributed computing on Spark.\n",
    "#### The steps followed are as below\n",
    "##### 1. First read the input text file to a Spark RDD\n",
    "##### 2. Split the words in each line based on space using **flatMap**\n",
    "##### 3. Create a pair RDD with content as (word, 1)\n",
    "##### 4. Using **reduceByKey** calculate the word count across RDD's efficiently\n",
    "##### 5. Finally, the RDD is sorted by key and written to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main tweet input which is used for both challenges\n",
    "file_name = 'tweets.txt'\n",
    "\n",
    "# Create an RDD by reading the input file\n",
    "tweets = sc.textFile(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the tweet based on white space\n",
    "# Create pair rdd of type (word, 1)\n",
    "# Compute the frequency of each word by reducing by key\n",
    "# Sort the rdd based on words ascii code\n",
    "# import timeit\n",
    "tweet_word_count_rdd = (tweets.flatMap(lambda s: s.split())\n",
    "                              .map(lambda s: (s, 1))\n",
    "                              .reduceByKey(lambda x, y: x + y)\n",
    "                              .sortByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the RDD based on unicode and write the result to the output file\n",
    "with open('ft1.txt','w') as f:\n",
    "    for line in tweet_word_count_rdd.collect():\n",
    "        f.write(line[0].encode('utf-8')+\" \"+str(line[1])+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2:  Rolling Median of the tweets**\n",
    "#### The Algorithm implemented to calculate rolling median is based on [Heap Data structure](https://en.wikipedia.org/wiki/Heap_(data_structure) is as below:\n",
    "##### 1. Two Heaps, one MaxHeap and one MinHeap is maintained. \n",
    "##### 2. Max Heap is used to represent elements that are less than effective median and Min Heap for those elements that are greater than the effective median\n",
    "##### 3. Each element is processed one by one and the size of the heaps differ by atmost 1\n",
    "##### 4. When both heaps contain same number of elements, median is calculated as the average of root from both the heaps\n",
    "##### 5. If the heaps are unbalanced, effective median is selected as the root of the heap consisting of more elements\n",
    "\n",
    "### Spark is used for calculating rolling median as below:\n",
    "##### 1. Creating max and min heap using python's **[heapq](https://docs.python.org/2/library/heapq.html)** library. (maxheap is simulated by adding -1 * value to the queue)\n",
    "##### 2. Creating broadcast variable for maxHeap, minHeap and current median, as this saves the process of sending the variables from driver to each of the worker nodes whenever the function is called.\n",
    "##### 3. The above algorithm is used to calculate rolling median and the final result is written to the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split the tweet based on blank space and get the length of unique words for that tweet\n",
    "def get_tweet_length(tweet):\n",
    "    \"\"\"\n",
    "    Get the count of unique words in a tweet.\n",
    "    Args: \n",
    "        tweet (str) - a single tweet.\n",
    "    Returns:\n",
    "        count of unique words in the tweet.\n",
    "    \"\"\"\n",
    "    tweets = tweet.split()\n",
    "    return len(set(tweets))\n",
    "\n",
    "# create an RDD consisting of length of unique word tweets from the original tweets RDD\n",
    "tweet_words_rdd = tweets.map(get_tweet_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create max and min heap using heapq library\n",
    "import heapq\n",
    "PRESENT_MEDIAN = [0]\n",
    "MAX_HEAP_LIST = []\n",
    "MIN_HEAP_LIST = []\n",
    "heapq.heapify(MIN_HEAP_LIST)\n",
    "heapq.heapify(MAX_HEAP_LIST)\n",
    "\n",
    "# Create broadcast variable for current median, min and max heaps\n",
    "current_median = sc.broadcast(PRESENT_MEDIAN)\n",
    "MAX_HEAP = sc.broadcast(MAX_HEAP_LIST)\n",
    "MIN_HEAP = sc.broadcast(MIN_HEAP_LIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the size of the heap\n",
    "def get_size():\n",
    "    \"\"\"\n",
    "    Get the relative counts of min_heap and max_heap\n",
    "    Returns:\n",
    "        0 - if both heaps are balanced\n",
    "        1 - max_heap contains more elements that min_heap\n",
    "        -1 - otherwise\n",
    "    \"\"\"\n",
    "    if len(MIN_HEAP.value) == len(MAX_HEAP.value):\n",
    "        return 0\n",
    "    elif len(MIN_HEAP.value) < len(MAX_HEAP.value):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# function to calcuate rolling median\n",
    "def get_median(val):\n",
    "    \"\"\"\n",
    "    Function to compute the rolling median for the tweets from input file.\n",
    "\n",
    "    Args:\n",
    "        val (int) - number of unique words per tweet\n",
    "        current_median (broadcast variable) - effective median after processing each tweet\n",
    "    Returns:\n",
    "        Rolling median of each tweet separated by new line\n",
    "    \"\"\"\n",
    "    size_diff = get_size()\n",
    "\n",
    "    # heaps are balanced\n",
    "    if size_diff == 0:\n",
    "        # if current word count is less than effective median\n",
    "        # both heaps consists of same number of elements\n",
    "        # Add new value to MAX_HEAP if value is less than current median\n",
    "        # else add the value to MIN_HEAP\n",
    "        # median is the top most elements of MIN_HEAP or MAX_HEAP\n",
    "\n",
    "        if val < current_median.value[-1]:\n",
    "            heapq.heappush(MAX_HEAP.value, val * -1)\n",
    "            current_median.value.append(heapq.nsmallest(1, MAX_HEAP.value)[0] * -1)\n",
    "            return current_median.value[-1]\n",
    "        else:\n",
    "            heapq.heappush(MIN_HEAP.value, val)\n",
    "            current_median.value.append(heapq.nsmallest(1, MIN_HEAP.value)[0])\n",
    "            return current_median.value[-1]\n",
    "\n",
    "    # MAX_HEAP has more elements than MIN_HEAP\n",
    "    elif size_diff == 1:\n",
    "        if val < current_median.value[-1]:\n",
    "            min_insert = heapq.heappop(MAX_HEAP.value)\n",
    "            heapq.heappush(MIN_HEAP.value, min_insert * -1)\n",
    "            heapq.heappush(MAX_HEAP.value, val * -1)\n",
    "        else:\n",
    "            heapq.heappush(MIN_HEAP.value, val)\n",
    "\n",
    "        # Balanced heaps\n",
    "        # Median is the average of root elements of MIN_HEAP and MAX_HEAP\n",
    "        min_heap_root = heapq.nsmallest(1, MIN_HEAP.value)[0]\n",
    "        max_heap_root = heapq.nsmallest(1, MAX_HEAP.value)[0] * -1\n",
    "        current_median.value.append((min_heap_root + max_heap_root) / (2 * 1.0))\n",
    "        return current_median.value[-1]\n",
    "\n",
    "    # MIN_HEAP has more elements than MAX_HEAP\n",
    "    else:\n",
    "        if val < current_median.value[-1]:\n",
    "            heapq.heappush(MAX_HEAP.value, val * -1)\n",
    "        else:\n",
    "            max_insert = heapq.heappop(MIN_HEAP.value)\n",
    "            heapq.heappush(MAX_HEAP.value, max_insert * -1)\n",
    "            heapq.heappush(MIN_HEAP.value, val)\n",
    "\n",
    "        # Balanced heaps\n",
    "        # Median is the average of root elements of MIN_HEAP and MAX_HEAP\n",
    "        min_heap_root = heapq.nsmallest(1, MIN_HEAP.value)[0]\n",
    "        max_heap_root = heapq.nsmallest(1, MAX_HEAP.value)[0] * -1\n",
    "        current_median.value.append((min_heap_root + max_heap_root) / (2 * 1.0))\n",
    "        return current_median.value[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the rolling median for each of the tweets from unique tweet words RDD\n",
    "rolling_median_rdd = tweet_words_rdd.map(get_median)\n",
    "with open('ft2.txt','w') as out_file:\n",
    "    for line in rolling_median_rdd.collect():\n",
    "        out_file.write(str(line)+\"\\n\")\n",
    "    out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
